# üåê D√©ploiement Ollama en Production

## Probl√®me

Ton site est en ligne (Vercel/Netlify), mais Ollama tourne en local sur ton Mac. Les utilisateurs ne peuvent pas y acc√©der.

## Solutions

---

## üéØ Option 1 : Serveur Ollama Cloud (Recommand√© - Simple)

### A. Utiliser Replicate.com (Pay-as-you-go)

**Avantages :**
- ‚úÖ Pas de serveur √† g√©rer
- ‚úÖ Pay-as-you-go (~$0.0001/seconde)
- ‚úÖ Scaling automatique
- ‚úÖ Deepseek disponible

**Configuration :**

1. **Cr√©e un compte sur [Replicate.com](https://replicate.com)**

2. **Modifie `/app/lib/ollama.ts` :**

```typescript
export async function queryOllama(
  messages: OllamaMessage[],
  options: {
    temperature?: number;
    format?: 'json';
  } = {}
): Promise<string> {
  try {
    const isProduction = process.env.NODE_ENV === 'production';
    
    if (isProduction) {
      // Utiliser Replicate en production
      const response = await fetch('https://api.replicate.com/v1/predictions', {
        method: 'POST',
        headers: {
          'Authorization': `Token ${process.env.REPLICATE_API_TOKEN}`,
          'Content-Type': 'application/json',
        },
        body: JSON.stringify({
          version: 'deepseek-ai/deepseek-v3', // Version du mod√®le
          input: {
            prompt: messages[messages.length - 1].content,
            system_prompt: messages[0].content,
            temperature: options.temperature || 0.7,
          }
        })
      });
      
      const data = await response.json();
      return data.output;
    } else {
      // Utiliser Ollama local en d√©veloppement
      // ... ton code actuel
    }
  } catch (error) {
    console.error('Erreur:', error);
    throw error;
  }
}
```

3. **Variables d'environnement (Vercel/Netlify) :**

```env
# Production
REPLICATE_API_TOKEN=r8_xxx...
USE_OLLAMA=true
NODE_ENV=production

# D√©veloppement (local)
USE_OLLAMA=true
OLLAMA_URL=http://localhost:11434
```

**Co√ªt estim√© :**
- ~$0.0001/seconde = $0.006/minute
- 1000 requ√™tes/mois ‚âà **$5-10/mois**

---

## üéØ Option 2 : VPS avec Ollama (Moins cher √† long terme)

### A. D√©ployer sur Hetzner (‚Ç¨4/mois)

**Avantages :**
- ‚úÖ Tr√®s bon rapport qualit√©/prix
- ‚úÖ Serveur d√©di√©
- ‚úÖ Contr√¥le total
- ‚úÖ Pas de limite d'utilisation

**√âtapes :**

1. **Cr√©e un serveur sur [Hetzner](https://www.hetzner.com)**
   - Choisis : CPX21 (3 vCPU, 4GB RAM) - ‚Ç¨4.51/mois
   - OS : Ubuntu 22.04

2. **Connecte-toi en SSH :**

```bash
ssh root@ton-ip-serveur
```

3. **Installe Ollama :**

```bash
# Installer Ollama
curl -fsSL https://ollama.com/install.sh | sh

# T√©l√©charger le mod√®le
ollama pull deepseek-v3.1:671b-cloud

# Configurer Ollama pour √©couter sur toutes les interfaces
sudo systemctl edit ollama

# Ajoute :
[Service]
Environment="OLLAMA_HOST=0.0.0.0:11434"

# Red√©marre
sudo systemctl restart ollama
```

4. **S√©curise avec Nginx + SSL :**

```bash
# Installer Nginx
sudo apt update
sudo apt install nginx certbot python3-certbot-nginx

# Configurer Nginx
sudo nano /etc/nginx/sites-available/ollama

# Contenu :
server {
    listen 80;
    server_name ollama.ton-domaine.com;

    location / {
        proxy_pass http://localhost:11434;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}

# Activer
sudo ln -s /etc/nginx/sites-available/ollama /etc/nginx/sites-enabled/
sudo nginx -t
sudo systemctl restart nginx

# SSL gratuit
sudo certbot --nginx -d ollama.ton-domaine.com
```

5. **Variables d'environnement (Vercel/Netlify) :**

```env
USE_OLLAMA=true
OLLAMA_URL=https://ollama.ton-domaine.com
```

**Co√ªt total :**
- Serveur Hetzner : **‚Ç¨4.51/mois**
- Domaine : **‚Ç¨10/an** (optionnel, tu peux utiliser l'IP)
- **Total : ~‚Ç¨5/mois**

---

## üéØ Option 3 : Modal.com (Serverless)

**Avantages :**
- ‚úÖ Serverless (pas de serveur √† g√©rer)
- ‚úÖ GPU disponible
- ‚úÖ Pay-as-you-go
- ‚úÖ Scaling automatique

**Configuration :**

1. **Cr√©e un compte sur [Modal.com](https://modal.com)**

2. **Cr√©e un endpoint Modal :**

```python
# modal_ollama.py
import modal

stub = modal.Stub("ollama-deepseek")

@stub.function(
    image=modal.Image.debian_slim().pip_install("ollama"),
    gpu="T4",  # GPU optionnel
)
@modal.web_endpoint(method="POST")
def chat(data: dict):
    import ollama
    
    response = ollama.chat(
        model='deepseek-v3.1:671b-cloud',
        messages=data['messages']
    )
    
    return {"content": response['message']['content']}
```

3. **D√©ploie :**

```bash
modal deploy modal_ollama.py
```

4. **Variables d'environnement :**

```env
USE_OLLAMA=true
OLLAMA_URL=https://ton-username--ollama-deepseek-chat.modal.run
```

**Co√ªt estim√© :**
- ~$0.0002/seconde avec GPU
- 1000 requ√™tes/mois ‚âà **$8-12/mois**

---

## üéØ Option 4 : Hybrid (Recommand√© pour √©conomiser)

**Strat√©gie intelligente :**

```typescript
// /app/lib/ollama.ts
const USE_OLLAMA = process.env.USE_OLLAMA === 'true';
const IS_PRODUCTION = process.env.NODE_ENV === 'production';
const OLLAMA_AVAILABLE = await checkOllamaAvailability();

if (USE_OLLAMA && OLLAMA_AVAILABLE) {
  // Utiliser Ollama (gratuit si disponible)
  content = await queryOllama(...);
} else if (IS_PRODUCTION) {
  // En production, utiliser Replicate (payant mais fiable)
  content = await queryReplicate(...);
} else {
  // Fallback vers Deepseek API
  content = await queryDeepseek(...);
}
```

**Configuration :**

```env
# Production (Vercel/Netlify)
USE_OLLAMA=false  # D√©sactiver Ollama en prod
REPLICATE_API_TOKEN=r8_xxx...
DEEPSEEK_API_KEY=sk-xxx...  # Fallback

# D√©veloppement (local)
USE_OLLAMA=true
OLLAMA_URL=http://localhost:11434
```

**Avantages :**
- ‚úÖ Gratuit en d√©veloppement (Ollama local)
- ‚úÖ Fiable en production (Replicate)
- ‚úÖ Fallback vers Deepseek si probl√®me
- ‚úÖ Co√ªt optimis√©

---

## üìä Comparaison des solutions

| Solution | Co√ªt/mois | Complexit√© | Fiabilit√© | Recommand√© pour |
|----------|-----------|------------|-----------|-----------------|
| **Replicate** | $5-10 | ‚≠ê Facile | ‚≠ê‚≠ê‚≠ê | D√©butants, MVP |
| **Hetzner VPS** | ‚Ç¨5 | ‚≠ê‚≠ê Moyen | ‚≠ê‚≠ê‚≠ê | Long terme, contr√¥le |
| **Modal.com** | $8-12 | ‚≠ê‚≠ê Moyen | ‚≠ê‚≠ê‚≠ê | Scaling, GPU |
| **Hybrid** | $5-15 | ‚≠ê‚≠ê‚≠ê Avanc√© | ‚≠ê‚≠ê‚≠ê | Optimisation co√ªts |
| **Deepseek API** | $50-100 | ‚≠ê Facile | ‚≠ê‚≠ê‚≠ê | Simplicit√© maximale |

---

## üöÄ Ma recommandation

### Pour commencer (court terme) :
**Option 4 : Hybrid**
- Ollama local en d√©veloppement (gratuit)
- Replicate en production (pay-as-you-go)
- Deepseek API en fallback

### Pour √©conomiser (long terme) :
**Option 2 : VPS Hetzner**
- ‚Ç¨5/mois fixe
- Illimit√©
- Contr√¥le total

---

## üìù Configuration finale recommand√©e

### 1. D√©veloppement (local)

```env
# .env.local
USE_OLLAMA=true
OLLAMA_URL=http://localhost:11434
NODE_ENV=development
```

### 2. Production (Vercel/Netlify)

```env
# Variables d'environnement Vercel/Netlify
USE_OLLAMA=false
REPLICATE_API_TOKEN=r8_xxx...
DEEPSEEK_API_KEY=sk-xxx...  # Fallback
NODE_ENV=production
```

### 3. Code adaptatif

Le code que j'ai d√©j√† cr√©√© g√®re automatiquement :
- ‚úÖ Ollama local en dev
- ‚úÖ Fallback vers API cloud si Ollama indisponible
- ‚úÖ Logs clairs pour debugging

---

## ‚ö° D√©ploiement rapide (5 minutes)

**Solution la plus rapide : Replicate**

1. Cr√©e un compte sur [Replicate.com](https://replicate.com)
2. Copie ton API token
3. Ajoute dans Vercel/Netlify :
   ```
   REPLICATE_API_TOKEN=r8_xxx...
   USE_OLLAMA=false
   ```
4. Modifie le code pour utiliser Replicate (je peux le faire si tu veux)
5. Red√©ploie ton site

**Co√ªt : ~$5-10/mois** au lieu de $50-100 avec Deepseek ! üéâ

---

Quelle solution pr√©f√®res-tu ? Je peux t'aider √† la mettre en place ! üöÄ
