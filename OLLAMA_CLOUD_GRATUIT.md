# ğŸ‰ Ollama Cloud - Solution 100% GRATUITE

## âœ… Pourquoi Ollama Cloud ?

- **$0** - ComplÃ¨tement gratuit (bÃªta)
- Pas de serveur Ã  gÃ©rer
- Fonctionne en production
- MÃªme API que Ollama local
- Deepseek disponible

---

## ğŸš€ Configuration (5 minutes)

### Ã‰tape 1 : CrÃ©er un compte Ollama Cloud

1. **Va sur [ollama.com](https://ollama.com)**
2. **Clique sur "Sign Up"** (ou "Get Started")
3. **CrÃ©e ton compte** (email + mot de passe)
4. **VÃ©rifie ton email**

### Ã‰tape 2 : Obtenir ton API Key

1. **Connecte-toi** sur [ollama.com](https://ollama.com)
2. **Va dans "Settings"** ou "API Keys"
3. **CrÃ©e une nouvelle clÃ© API**
4. **Copie la clÃ©** (elle ressemble Ã  : `ollama_xxxxxxxxxxxxx`)

---

## ğŸ”§ Configuration du projet

### 1. Variables d'environnement LOCAL (.env.local)

Ton fichier `.env.local` actuel (pour dÃ©veloppement) :

```env
# Ollama local (dÃ©veloppement - gratuit)
USE_OLLAMA=true
OLLAMA_URL=http://localhost:11434
NODE_ENV=development

# Ollama Cloud (production - gratuit)
OLLAMA_API_KEY=ollama_xxxxxxxxxxxxx
OLLAMA_CLOUD_URL=https://api.ollama.com

# Autres configs...
NEXT_PUBLIC_SUPABASE_URL=...
DEEPSEEK_API_KEY=...  # Garde en fallback
```

### 2. Variables d'environnement PRODUCTION (Vercel/Netlify)

Dans ton dashboard Vercel ou Netlify, ajoute :

```env
USE_OLLAMA=true
OLLAMA_API_KEY=ollama_xxxxxxxxxxxxx
OLLAMA_CLOUD_URL=https://api.ollama.com
NODE_ENV=production
```

**Important :** Supprime ou laisse vide `OLLAMA_URL` en production (il utilisera automatiquement Ollama Cloud).

---

## ğŸ“ Comment Ã§a marche maintenant

### En dÃ©veloppement (ton Mac)
```
âœ… Utilise Ollama local (http://localhost:11434)
âœ… Gratuit
âœ… Rapide
```

### En production (site en ligne)
```
âœ… Utilise Ollama Cloud (https://api.ollama.com)
âœ… Gratuit
âœ… Accessible partout
```

### Fallback automatique
```
Si Ollama Cloud ne rÃ©pond pas â†’ Utilise API Deepseek (payant)
```

---

## ğŸ§ª Test

### 1. Tester en local

```bash
# Assure-toi qu'Ollama local tourne
ollama serve

# Lance ton serveur
npm run dev

# Teste une fonctionnalitÃ© IA
# Tu devrais voir dans les logs :
# "Utilisation d'Ollama (local - gratuit)"
```

### 2. Tester Ollama Cloud directement

```bash
# Test avec curl
curl https://api.ollama.com/api/chat \
  -H "Authorization: Bearer ollama_xxxxxxxxxxxxx" \
  -H "Content-Type: application/json" \
  -d '{
    "model": "deepseek-v3.1:671b-cloud",
    "messages": [
      {
        "role": "user",
        "content": "Dis bonjour"
      }
    ]
  }'
```

Si Ã§a fonctionne, tu verras une rÃ©ponse JSON avec le message.

---

## ğŸš€ DÃ©ploiement sur Vercel/Netlify

### Vercel

1. **Va sur [vercel.com](https://vercel.com)**
2. **SÃ©lectionne ton projet**
3. **Settings â†’ Environment Variables**
4. **Ajoute :**
   ```
   USE_OLLAMA=true
   OLLAMA_API_KEY=ollama_xxxxxxxxxxxxx
   OLLAMA_CLOUD_URL=https://api.ollama.com
   NODE_ENV=production
   ```
5. **RedÃ©ploie** (Deployments â†’ Redeploy)

### Netlify

1. **Va sur [netlify.com](https://netlify.com)**
2. **SÃ©lectionne ton site**
3. **Site settings â†’ Environment variables**
4. **Ajoute :**
   ```
   USE_OLLAMA=true
   OLLAMA_API_KEY=ollama_xxxxxxxxxxxxx
   OLLAMA_CLOUD_URL=https://api.ollama.com
   NODE_ENV=production
   ```
5. **RedÃ©ploie** (Deploys â†’ Trigger deploy)

---

## ğŸ“Š VÃ©rification

### Logs Ã  surveiller en production

Dans les logs Vercel/Netlify, tu devrais voir :

```
âœ… Utilisation d'Ollama (local - gratuit) pour lettre de motivation
```

Ou si tu as configurÃ© les logs pour montrer l'URL :

```
âœ… Utilisation d'Ollama Cloud (https://api.ollama.com)
```

### Si tu vois encore "Insufficient Balance"

Cela signifie que :
1. âŒ `OLLAMA_API_KEY` n'est pas configurÃ©e
2. âŒ `USE_OLLAMA` n'est pas Ã  `true`
3. âŒ Le serveur n'a pas Ã©tÃ© redÃ©ployÃ©

**Solution :** VÃ©rifie les variables d'environnement et redÃ©ploie.

---

## ğŸ’¡ Avantages de cette solution

| Aspect | Avant (Deepseek API) | Maintenant (Ollama Cloud) |
|--------|---------------------|---------------------------|
| **CoÃ»t** | $50-100/mois | **$0** ğŸ‰ |
| **Limite** | Selon crÃ©dit | IllimitÃ© (bÃªta) |
| **Setup** | Simple | Simple |
| **Maintenance** | Aucune | Aucune |
| **FiabilitÃ©** | â­â­â­ | â­â­â­ |

---

## âš ï¸ Important Ã  savoir

### Ollama Cloud est en bÃªta

- âœ… **Actuellement gratuit**
- âš ï¸ Pourrait devenir payant plus tard (mais restera probablement moins cher que Deepseek)
- âœ… Tu as le fallback vers Deepseek API si besoin
- âœ… Tu peux toujours revenir Ã  Ollama local en dev

### Limites potentielles

- Peut avoir des limites de rate (requÃªtes/minute)
- Peut Ãªtre plus lent que local
- DÃ©pend d'internet

Mais pour l'instant, c'est **100% gratuit** ! ğŸ‰

---

## ğŸ¯ RÃ©capitulatif

### Ce que tu dois faire :

1. âœ… CrÃ©er un compte sur [ollama.com](https://ollama.com)
2. âœ… RÃ©cupÃ©rer ton API key
3. âœ… Ajouter `OLLAMA_API_KEY` dans `.env.local`
4. âœ… Ajouter les variables dans Vercel/Netlify
5. âœ… RedÃ©ployer ton site

### RÃ©sultat :

- ğŸ’° **$0/mois** au lieu de $50-100
- ğŸš€ Fonctionne en production
- ğŸ”§ Pas de serveur Ã  gÃ©rer
- âœ… Fallback automatique si problÃ¨me

---

## ğŸ†˜ Besoin d'aide ?

Si tu as des problÃ¨mes :

1. VÃ©rifie que ton API key est valide
2. VÃ©rifie les logs Vercel/Netlify
3. Teste l'API directement avec curl
4. VÃ©rifie que `NODE_ENV=production` en production

---

**Tu es prÃªt ! Plus de soucis de budget IA !** ğŸ‰
