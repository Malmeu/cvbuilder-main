# ğŸš€ Migration vers Ollama - Guide Complet

## âœ… Ce qui est fait

### 1. Service Ollama crÃ©Ã©
- âœ… `/app/lib/ollama.ts` - Service wrapper pour Ollama
- âœ… Fonctions: `queryOllama`, `checkOllamaAvailability`, `queryDeepseekViaOllama`

### 2. Route de test crÃ©Ã©e
- âœ… `/app/api/test-ollama/route.ts` - Pour tester la connexion Ollama

### 3. TOUTES les routes IA migrÃ©es avec fallback
- âœ… `/app/api/analyze-cv/route.ts` - Analyse de CV
- âœ… `/app/api/generate-cover-letter/route.ts` - GÃ©nÃ©ration de lettre de motivation
- âœ… `/app/api/visa-interview/route.ts` - Entretien visa
- âœ… `/app/api/interview-prep/route.ts` - PrÃ©paration d'entretien
- âœ… `/app/api/blog/generate/route.ts` - GÃ©nÃ©ration d'articles blog

### 4. Fonction client migrÃ©e
- âœ… `/app/lib/ai.ts` - `generateCoverLetter` appelle maintenant la route API

---

## ğŸ“ Configuration requise

Ajoute ces variables dans ton `.env.local` :

```env
# Ollama (local - gratuit)
USE_OLLAMA=true
OLLAMA_URL=http://localhost:11434

# Deepseek Cloud (fallback si Ollama ne fonctionne pas)
DEEPSEEK_API_KEY=<ta_clÃ©_existante>
```

---

## ğŸ§ª Tests Ã  effectuer

### 1. Tester Ollama directement
```bash
# VÃ©rifier qu'Ollama tourne
curl http://localhost:11434/api/tags

# Tester via l'API Next.js
curl http://localhost:3000/api/test-ollama
```

### 2. Tester l'analyse CV
- Va sur la page d'analyse CV
- Upload un CV
- VÃ©rifie les logs dans la console : tu devrais voir "Utilisation d'Ollama (local - gratuit)"

---

## ğŸ‰ Migration terminÃ©e !

### Toutes les routes ont Ã©tÃ© migrÃ©es :

1. **âœ… `/app/api/analyze-cv/route.ts`** - Analyse de CV
2. **âœ… `/app/api/generate-cover-letter/route.ts`** - Lettre de motivation
3. **âœ… `/app/api/visa-interview/route.ts`** - Entretien visa
4. **âœ… `/app/api/interview-prep/route.ts`** - PrÃ©paration entretien
5. **âœ… `/app/api/blog/generate/route.ts`** - GÃ©nÃ©ration d'articles blog

### Pattern de migration (copier-coller) :

```typescript
import { queryDeepseekViaOllama, checkOllamaAvailability } from '@/app/lib/ollama';

const USE_OLLAMA = process.env.USE_OLLAMA === 'true';

// Dans ta fonction :
let response: string;

if (USE_OLLAMA) {
  console.log('Utilisation d\'Ollama (local - gratuit)');
  
  const isAvailable = await checkOllamaAvailability();
  if (!isAvailable) {
    throw new Error('Ollama non disponible');
  }

  response = await queryDeepseekViaOllama(
    systemPrompt,
    userPrompt,
    { 
      temperature: 0.7,
      format: 'json' // Optionnel, pour les rÃ©ponses JSON
    }
  );
} else {
  console.log('Utilisation de l\'API Deepseek Cloud (payant)');
  // Ton code API existant...
}
```

---

## ğŸ’° Ã‰conomies estimÃ©es

### Avant (API Deepseek Cloud) :
- Analyse CV : ~800 tokens Ã— $0.14/M = $0.00011 par analyse
- Entretien visa : ~500 tokens Ã— 5 questions = $0.00035 par entretien
- **CoÃ»t mensuel estimÃ©** : $50-100 (selon usage)

### AprÃ¨s (Ollama local) :
- **CoÃ»t : $0** ğŸ‰
- Seul coÃ»t : Ã©lectricitÃ© de ton Mac (nÃ©gligeable)

---

## âš¡ Performances

### Deepseek Cloud :
- Latence : 2-5 secondes (rÃ©seau)
- DÃ©bit : LimitÃ© par l'API

### Ollama Local (deepseek-v3.1:671b-cloud) :
- Latence : Variable selon ton Mac
- DÃ©bit : IllimitÃ©
- **Note** : C'est un modÃ¨le cloud-hosted via Ollama, donc il y a quand mÃªme une connexion rÃ©seau mais sans coÃ»t

---

## ğŸ”§ DÃ©pannage

### Ollama ne rÃ©pond pas
```bash
# RedÃ©marrer Ollama
ollama serve

# VÃ©rifier les modÃ¨les
ollama list
```

### Le modÃ¨le est lent
- Le modÃ¨le `deepseek-v3.1:671b-cloud` est hÃ©bergÃ© sur les serveurs Ollama
- C'est gratuit mais nÃ©cessite une connexion internet
- Si tu veux du 100% local, utilise plutÃ´t `mistral:latest` (dÃ©jÃ  installÃ©)

### Erreur "Ollama non disponible"
- Assure-toi qu'Ollama tourne : `ollama serve`
- VÃ©rifie l'URL dans `.env.local` : `OLLAMA_URL=http://localhost:11434`

---

## ğŸ¯ Prochaines Ã©tapes recommandÃ©es

1. **Tester l'analyse CV** avec Ollama
2. **Si Ã§a marche bien** â†’ Migrer les autres routes une par une
3. **Monitorer les performances** et ajuster si besoin
4. **Garder le fallback** vers l'API cloud pour la production

---

## ğŸ“Œ Notes importantes

- âœ… Le fallback vers l'API cloud est automatique si Ollama plante
- âœ… Tu peux basculer entre les deux en changeant `USE_OLLAMA` dans `.env.local`
- âœ… Aucune modification du code client nÃ©cessaire
- âš ï¸ Le modÃ¨le `deepseek-v3.1:671b-cloud` nÃ©cessite internet (hÃ©bergÃ© par Ollama)
- ğŸ’¡ Pour du 100% local, utilise `mistral:latest` Ã  la place
